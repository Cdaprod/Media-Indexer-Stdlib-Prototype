#!/usr/bin/env python3
# ---------------------------------------------------------------------------
# Uploader routes â€“ multipart file â†’ _INCOMING/web â†’ background ingest
# ---------------------------------------------------------------------------

from __future__ import annotations

import logging
from pathlib import Path
from typing import List, Optional

from fastapi import (
    APIRouter,
    BackgroundTasks,
    File,
    Form,
    HTTPException,
    UploadFile,
)

from video.core.ingest import ingest_files       # ðŸ”¸ background worker
from video.config      import WEB_UPLOADS        # Path("/data/_INCOMING/web")

log     = logging.getLogger("video.uploader")
router  = APIRouter(prefix="/api/v1/upload", tags=["upload"])

# â”€â”€ ensure staging dir exists once at import-time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WEB_UPLOADS.mkdir(parents=True, exist_ok=True)
log.info("Uploader ready â€“ staging dir: %s", WEB_UPLOADS)


@router.post("/", summary="Upload 1-N video files as a new batch")
async def upload_batch(
    bg    : BackgroundTasks,
    files : List[UploadFile] = File(...),
    batch : Optional[str]    = Form(None),
) -> dict:
    """
    1. Streams each â€¹UploadFileâ€º into *_INCOMING/web/* (non-blocking).
    2. Queues ``ingest_files()`` which:
         â€¢ hashes & relocates to /data/media
         â€¢ creates / updates batch manifest
    3. Returns immediately with a Â«queuedÂ» response.
    """
    if not files:
        raise HTTPException(status_code=400, detail="no files sent")

    saved: list[Path] = []

    # â”€â”€ 1) stream uploads to disk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for f in files:
        tgt = WEB_UPLOADS / f.filename
        try:
            with tgt.open("wb") as out:
                while chunk := await f.read(1 << 20):      # 1 MiB chunks
                    out.write(chunk)
            saved.append(tgt)
            log.info("â¬† %s â†’ %s  (%s bytes)", f.filename, tgt, tgt.stat().st_size)
        finally:
            await f.close()

    # â”€â”€ 2) kick off ingest worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    bg.add_task(ingest_files, saved, batch_name=batch)
    log.info("Queued ingest for %d file(s)  â€“ batch=%s", len(saved), batch)

    # â”€â”€ 3) immediate response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    return {
        "status": "queued",
        "batch" : batch,
        "files" : [p.name for p in saved],
    }